{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "searching-sauce",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None\n",
    "import numpy as np\n",
    "\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Embedding, LSTM, Dropout, Activation, TimeDistributed, BatchNormalization, Conv1D, MaxPooling1D, Flatten, Bidirectional\n",
    "from keras.layers.merge import concatenate\n",
    "from keras.preprocessing import sequence\n",
    "from tensorflow.keras.models import Model\n",
    "from tensorflow.keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from tensorflow.keras.optimizers import Adam\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.model_selection import StratifiedKFold\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.metrics import precision_score\n",
    "from sklearn.metrics import recall_score\n",
    "from sklearn.metrics import f1_score\n",
    "from sklearn.metrics import confusion_matrix\n",
    "\n",
    "import os, sys\n",
    "module_path = os.path.abspath(os.path.join('../..'))\n",
    "if module_path not in sys.path:\n",
    "    sys.path.append(module_path)\n",
    "from util.dataprep import get_vectors, get_data\n",
    "\n",
    "import tensorflow as tf\n",
    "import matplotlib.pyplot as plt \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "virtual-enhancement",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train labels:  Counter({1: 3056, 0: 3055})\n",
      "Test labels:  Counter({1: 873, 0: 873})\n",
      "Val labels:  Counter({0: 437, 1: 436})\n"
     ]
    }
   ],
   "source": [
    "# To calculate on GPU\n",
    "# https://github.com/tensorflow/tensorflow/issues/33721\n",
    "TF_FORCE_GPU_ALLOW_GROWTH=1\n",
    "physical_devices = tf.config.list_physical_devices('GPU')\n",
    "tf.config.experimental.set_memory_growth(physical_devices[0], True)\n",
    "import os\n",
    "os.environ[\"TF_FORCE_GPU_ALLOW_GROWTH\"]=\"true\"\n",
    "# CONSTANTS\n",
    "seed = 42\n",
    "X_train, y_train, X_test, y_test, X_val, y_val, vocab_size, emdedding_size, vectors = get_data()\n",
    "train_max = X_train.max()\n",
    "test_max = X_test.max()\n",
    "val_max = X_val.max()\n",
    "max_all = max([train_max, test_max, val_max])\n",
    "input_dim = max_all+1\n",
    "output_dim = 32\n",
    "# check if the data is evenly split\n",
    "import collections\n",
    "print('Train labels: ', collections.Counter(y_train))\n",
    "print('Test labels: ', collections.Counter(y_test))\n",
    "print('Val labels: ', collections.Counter(y_val))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "guided-render",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Early stopping setup\n",
    "early_stopping = EarlyStopping(monitor='val_loss', patience=4)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "generous-shareware",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_history(history_arrs, train, val, xlabel, ylabel, plot_title):\n",
    "    if len(history_arrs) == 1:\n",
    "        history = history_arrs[0]\n",
    "        plt.plot(history['accuracy'])\n",
    "        plt.plot(history['val_accuracy'])\n",
    "        plt.title(plot_title)\n",
    "        plt.ylabel(ylabel)\n",
    "        plt.xlabel(xlabel)\n",
    "        plt.legend(['Train', 'Validation'], loc='upper left')\n",
    "        plt.show()\n",
    "    else:\n",
    "        f, a = plt.subplots(1, len(history_arrs), figsize=(10,5))\n",
    "        for idx, history in enumerate(history_arrs):\n",
    "            # For Sine Function\n",
    "            a[idx].plot(history[train])\n",
    "            a[idx].plot(history[val])\n",
    "            title = plot_title + ' ' + str(idx)\n",
    "            a[idx].set_title(title)\n",
    "            a[idx].set_xlabel(xlabel)\n",
    "            a[idx].set_ylabel(ylabel)\n",
    "            a[idx].legend(['Train', 'Validation'], loc='upper left')\n",
    "        f.tight_layout()\n",
    "        plt.show()\n",
    "\n",
    "def calculate_metrics(model, X_test, y_test):\n",
    "    ypred_class = model.predict_classes(X_test, verbose=0)\n",
    "    ypred_class = ypred_class[:, 0]\n",
    "    accuracy = accuracy_score(y_test, ypred_class)\n",
    "    precision = precision_score(y_test, ypred_class)\n",
    "    recall = recall_score(y_test, ypred_class)\n",
    "    f1 = f1_score(y_test, ypred_class)\n",
    "    conf_matrix = confusion_matrix(y_test, ypred_class)\n",
    "    return accuracy, precision, recall, f1, conf_matrix\n",
    "\n",
    "def print_conf_matrix(conf_matrix):\n",
    "    cm = pd.DataFrame(\n",
    "    conf_matrix, \n",
    "    index=['true:positive', 'true:negative'], \n",
    "    columns=['pred:positive', 'pred:negative']\n",
    "    )\n",
    "    print(cm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "intermediate-indiana",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_lstm(embedding_layer, lr):\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(LSTM(128))\n",
    "    model.add(Dense(32))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=adamOptimizer, metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "stupid-zoning",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_bilstm(embedding_layer, lr):\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Bidirectional(LSTM(128)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=adamOptimizer, metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "billion-detective",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cnnlstm(embedding_layer, lr):\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(128, 2))\n",
    "    model.add(Conv1D(64, 2))\n",
    "    model.add(MaxPooling1D(pool_size=2))\n",
    "    model.add(LSTM(64))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=Adam(lr=lr), metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "level-distinction",
   "metadata": {},
   "outputs": [],
   "source": [
    "def model_cnnbilstm(embedding_layer, lr):\n",
    "    model = Sequential()\n",
    "    model.add(embedding_layer)\n",
    "    model.add(Conv1D(128, 2))\n",
    "    model.add(Conv1D(64, 2))\n",
    "    model.add(MaxPooling1D(2))\n",
    "    model.add(Bidirectional(LSTM(128)))\n",
    "    model.add(Dense(1, activation='sigmoid'))\n",
    "    model.compile(loss = 'binary_crossentropy', optimizer=Adam(lr=lr), metrics = ['accuracy'])\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "embedded-sending",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Gridsearch parameters\n",
    "epochs = 20\n",
    "lr = [0.1, 0.01, 0.001, 0.0001]\n",
    "batch = [16, 32, 64]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "concerned-bottom",
   "metadata": {},
   "source": [
    "#### Girdsearch for LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "widespread-carry",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for learning rate:  0.1 , batch size:  16 , accuracy:  0.7119129300117493\n",
      "Running test for learning rate:  0.1 , batch size:  32 , accuracy:  0.6958763003349304\n",
      "Running test for learning rate:  0.1 , batch size:  64 , accuracy:  0.5\n",
      "Running test for learning rate:  0.01 , batch size:  16 , accuracy:  0.8613975048065186\n",
      "Running test for learning rate:  0.01 , batch size:  32 , accuracy:  0.8711340427398682\n",
      "Running test for learning rate:  0.01 , batch size:  64 , accuracy:  0.8722795248031616\n",
      "Running test for learning rate:  0.001 , batch size:  16 , accuracy:  0.8688430786132812\n",
      "Running test for learning rate:  0.001 , batch size:  32 , accuracy:  0.8642611503601074\n",
      "Running test for learning rate:  0.001 , batch size:  64 , accuracy:  0.876288652420044\n",
      "Running test for learning rate:  0.0001 , batch size:  16 , accuracy:  0.882588803768158\n",
      "Running test for learning rate:  0.0001 , batch size:  32 , accuracy:  0.8774341344833374\n",
      "Running test for learning rate:  0.0001 , batch size:  64 , accuracy:  0.8854524493217468\n",
      "Best performing paramters:\n",
      "Learning rate: 0.0001, batch size: 64\n"
     ]
    }
   ],
   "source": [
    "best_result = []\n",
    "best_acc = 0\n",
    "for lrate in lr:\n",
    "    for b in batch:\n",
    "        embedding = Embedding(input_dim, output_dim, trainable = True)\n",
    "        model = model_lstm(embedding, lrate)\n",
    "        history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=b, verbose = 0, callbacks=[early_stopping])\n",
    "        loss, acc = model.evaluate(X_test, y_test, verbose = 0, batch_size = b)\n",
    "        print('Running test for learning rate: ', lrate, ', batch size: ', b, ', accuracy: ', acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_result = [lrate, b]\n",
    "print('Best performing paramters:')\n",
    "print('Learning rate: %s, batch size: %s' % (best_result[0], best_result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "wound-token",
   "metadata": {},
   "source": [
    "#### Gridsearch for BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "indirect-marsh",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for learning rate:  0.1 , batch size:  16 , accuracy:  0.5\n",
      "Running test for learning rate:  0.1 , batch size:  32 , accuracy:  0.7623138427734375\n",
      "Running test for learning rate:  0.1 , batch size:  64 , accuracy:  0.7972508668899536\n",
      "Running test for learning rate:  0.01 , batch size:  16 , accuracy:  0.8705613017082214\n",
      "Running test for learning rate:  0.01 , batch size:  32 , accuracy:  0.869415819644928\n",
      "Running test for learning rate:  0.01 , batch size:  64 , accuracy:  0.8722795248031616\n",
      "Running test for learning rate:  0.001 , batch size:  16 , accuracy:  0.8310424089431763\n",
      "Running test for learning rate:  0.001 , batch size:  32 , accuracy:  0.8676975965499878\n",
      "Running test for learning rate:  0.001 , batch size:  64 , accuracy:  0.8797250986099243\n",
      "Running test for learning rate:  0.0001 , batch size:  16 , accuracy:  0.8877434134483337\n",
      "Running test for learning rate:  0.0001 , batch size:  32 , accuracy:  0.8865979313850403\n",
      "Running test for learning rate:  0.0001 , batch size:  64 , accuracy:  0.8791523575782776\n",
      "Best performing paramters:\n",
      "Learning rate: 0.0001, batch size: 16\n"
     ]
    }
   ],
   "source": [
    "best_result = []\n",
    "best_acc = 0\n",
    "for lrate in lr:\n",
    "    for b in batch:\n",
    "        embedding = Embedding(input_dim, output_dim, trainable = True)\n",
    "        model = model_bilstm(embedding, lrate)\n",
    "        history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=b, verbose = 0, callbacks=[early_stopping])\n",
    "        loss, acc = model.evaluate(X_test, y_test, verbose = 0, batch_size = b)\n",
    "        print('Running test for learning rate: ', lrate, ', batch size: ', b, ', accuracy: ', acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_result = [lrate, b]\n",
    "print('Best performing paramters:')\n",
    "print('Learning rate: %s, batch size: %s' % (best_result[0], best_result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "automatic-closure",
   "metadata": {},
   "source": [
    "#### Gridsearch for CNN-LSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "involved-float",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for learning rate:  0.1 , batch size:  16 , accuracy:  0.5383734107017517\n",
      "Running test for learning rate:  0.1 , batch size:  32 , accuracy:  0.5034364461898804\n",
      "Running test for learning rate:  0.1 , batch size:  64 , accuracy:  0.5234822630882263\n",
      "Running test for learning rate:  0.01 , batch size:  16 , accuracy:  0.6443299055099487\n",
      "Running test for learning rate:  0.01 , batch size:  32 , accuracy:  0.7754868268966675\n",
      "Running test for learning rate:  0.01 , batch size:  64 , accuracy:  0.8459335565567017\n",
      "Running test for learning rate:  0.001 , batch size:  16 , accuracy:  0.8373425006866455\n",
      "Running test for learning rate:  0.001 , batch size:  32 , accuracy:  0.8608247637748718\n",
      "Running test for learning rate:  0.001 , batch size:  64 , accuracy:  0.8436425924301147\n",
      "Running test for learning rate:  0.0001 , batch size:  16 , accuracy:  0.8676975965499878\n",
      "Running test for learning rate:  0.0001 , batch size:  32 , accuracy:  0.8659793734550476\n",
      "Running test for learning rate:  0.0001 , batch size:  64 , accuracy:  0.8619702458381653\n",
      "Best performing paramters:\n",
      "Learning rate: 0.0001, batch size: 16\n"
     ]
    }
   ],
   "source": [
    "best_result = []\n",
    "best_acc = 0\n",
    "for lrate in lr:\n",
    "    for b in batch:\n",
    "        embedding = Embedding(input_dim, output_dim, trainable = True)\n",
    "        model = model_cnnlstm(embedding, lrate)\n",
    "        history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=b, verbose = 0, callbacks=[early_stopping])\n",
    "        loss, acc = model.evaluate(X_test, y_test, verbose = 0, batch_size = b)\n",
    "        print('Running test for learning rate: ', lrate, ', batch size: ', b, ', accuracy: ', acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_result = [lrate, b]\n",
    "print('Best performing paramters:')\n",
    "print('Learning rate: %s, batch size: %s' % (best_result[0], best_result[1]))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "casual-tiger",
   "metadata": {},
   "source": [
    "#### Gridsearch for CNN-BiLSTM model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "fantastic-gasoline",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running test for learning rate:  0.1 , batch size:  16 , accuracy:  0.5171821117401123\n",
      "Running test for learning rate:  0.1 , batch size:  32 , accuracy:  0.5229095220565796\n",
      "Running test for learning rate:  0.1 , batch size:  64 , accuracy:  0.4977090358734131\n",
      "Running test for learning rate:  0.01 , batch size:  16 , accuracy:  0.6706758141517639\n",
      "Running test for learning rate:  0.01 , batch size:  32 , accuracy:  0.699312686920166\n",
      "Running test for learning rate:  0.01 , batch size:  64 , accuracy:  0.8344787955284119\n",
      "Running test for learning rate:  0.001 , batch size:  16 , accuracy:  0.8522336483001709\n",
      "Running test for learning rate:  0.001 , batch size:  32 , accuracy:  0.832187831401825\n",
      "Running test for learning rate:  0.001 , batch size:  64 , accuracy:  0.8442153334617615\n",
      "Running test for learning rate:  0.0001 , batch size:  16 , accuracy:  0.863115668296814\n",
      "Running test for learning rate:  0.0001 , batch size:  32 , accuracy:  0.8556700944900513\n",
      "Running test for learning rate:  0.0001 , batch size:  64 , accuracy:  0.8636884093284607\n",
      "Best performing paramters:\n",
      "Learning rate: 0.0001, batch size: 64\n"
     ]
    }
   ],
   "source": [
    "best_result = []\n",
    "best_acc = 0\n",
    "for lrate in lr:\n",
    "    for b in batch:\n",
    "        embedding = Embedding(input_dim, output_dim, trainable = True)\n",
    "        model = model_cnnbilstm(embedding, lrate)\n",
    "        history = model.fit(X_train, y_train, validation_data=(X_val, y_val), epochs=30, batch_size=b, verbose = 0, callbacks=[early_stopping])\n",
    "        loss, acc = model.evaluate(X_test, y_test, verbose = 0, batch_size = b)\n",
    "        print('Running test for learning rate: ', lrate, ', batch size: ', b, ', accuracy: ', acc)\n",
    "        if acc > best_acc:\n",
    "            best_acc = acc\n",
    "            best_result = [lrate, b]\n",
    "print('Best performing paramters:')\n",
    "print('Learning rate: %s, batch size: %s' % (best_result[0], best_result[1]))"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
